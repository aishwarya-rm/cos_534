{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which prediction problem to study\n",
    "task = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 1:\n",
    "    prediction = 'deceased'\n",
    "    df = pd.read_csv('/n/fs/fgvc/cos534/data/melanoma_allFrames.csv')\n",
    "    # dem = pd.read_csv(\"/n/fs/fgvc/cos534/data/melanoma_cohort.csv\")\n",
    "if task == 2:\n",
    "    prediction = 'deceased'\n",
    "    df = pd.read_csv('/n/fs/fgvc/cos534/data/hd_allFrames.csv')\n",
    "    # dem = pd.read_csv(\"/n/fs/fgvc/cos534/data/heart_cohort.csv\")\n",
    "if task == 3:\n",
    "    prediction = 'transplant'\n",
    "    df = pd.read_csv('/n/fs/fgvc/cos534/data/kidney_allFrames.csv')\n",
    "    # dem = pd.read_csv(\"/n/fs/fgvc/cos534/data/kidney_cohort.csv\")\n",
    "if task == 4:\n",
    "    prediction = 'ventilated'\n",
    "    df = pd.read_csv('/n/fs/fgvc/cos534/data/flu_pneum_allFrames.csv')\n",
    "    df = df.drop(columns=['vent'])\n",
    "    # dem = pd.read_csv(\"/n/fs/fgvc/cos534/data/flu_cohort.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv(\"/n/fs/fgvc/mimiciv/1.0/icu/icustays.csv\")\n",
    "admissions = pd.read_csv(\"/n/fs/fgvc/mimiciv/1.0/core/admissions.csv\")\n",
    "diagnoses_icd = pd.read_csv(\"/n/fs/fgvc/mimiciv/1.0/hosp/diagnoses_icd.csv\")\n",
    "procs = pd.read_csv(\"/n/fs/fgvc/mimiciv/1.0/icu/procedureevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction == 'transplant':\n",
    "    TRANSPLANT_CODES = ['0091', '0092', '0093', '02Y', '02YA0Z0', '02YA0Z1', \n",
    "                        '02YA0Z2', '0794', '07Y', '07YM0Z0', '07YM0Z1', '07YM0Z2', \n",
    "                        '07YP0Z0', '07YP0Z1', '07YP0Z2', '0BYM0Z0', '0BYM0Z1', \n",
    "                        '0BY', '0BYC0Z0', '0BYC0Z1', '0BYC0Z2', '0BYD0Z0', '0BYD0Z1', \n",
    "                        '0BYD0Z2', '0BYF0Z0', '0BYF0Z1', '0BYF0Z2', '0BYG0Z0', '0BYG0Z1', \n",
    "                        '0BYG0Z2', '0BYH0Z0', '0BYH0Z1', '0BYH0Z2', '0BYJ0Z0', '0BYJ0Z1',\n",
    "                        '0BYJ0Z2', '0BYK0Z0', '0BYK0Z1', '0BYK0Z2', '0BYL0Z0', '0BYL0Z1', \n",
    "                        '0BYL0Z2', '0BYM0Z2', '0DY', '0DY50Z0', '0DY50Z1', '0DY50Z2', \n",
    "                        '0DY60Z0', '0DY60Z1', '0DY60Z2', '0DY80Z0', '0DY80Z1', '0DY80Z2',\n",
    "                        '0DYE0Z0', '0DYE0Z1', '0DYE0Z2', '0FY', '0FY00Z0', '0FY00Z1', \n",
    "                        '0FY00Z2', '0FYG0Z0', '0FYG0Z1', '0FYG0Z2', '0TY', '0TY00Z0',\n",
    "                        '0TY00Z1', '0TY00Z2', '0TY10Z0', '0TY10Z1', '0TY10Z2', '0UY',\n",
    "                        '0UY00Z0', '0UY00Z1', '0UY00Z2', '0UY10Z0', '0UY10Z1', '0UY10Z2', \n",
    "                        '0UY90Z0', '0UY90Z1', '0UY90Z2', '0WY', '0WY20Z0', '0WY20Z1', '0XY', \n",
    "                        '0XYJ0Z0', '0XYJ0Z1', '0XYK0Z0', '0XYK0Z1', '10Y', '10Y03ZE', \n",
    "                        '10Y03ZF', '10Y03ZG', '10Y03ZH', '10Y03ZJ', '10Y03ZK', '10Y03ZL', \n",
    "                        '10Y03ZM', '10Y03ZN', '10Y03ZP', '10Y03ZQ', '10Y03ZR', '10Y03ZS', \n",
    "                        '10Y03ZT', '10Y03ZV', '10Y03ZY', '10Y04ZE', '10Y04ZF', '10Y04ZG', \n",
    "                        '10Y04ZH', '10Y04ZJ', '10Y04ZK', '10Y04ZL', '10Y04ZM', '10Y04ZN', \n",
    "                        '10Y04ZP', '10Y04ZQ', '10Y04ZR', '10Y04ZS', '10Y04ZT', '10Y04ZV', \n",
    "                        '10Y04ZY', '10Y07ZE', '10Y07ZF', '10Y07ZG', '10Y07ZH', '10Y07ZJ', \n",
    "                        '10Y07ZK', '10Y07ZL', '10Y07ZM', '10Y07ZN', '10Y07ZP', '10Y07ZQ', \n",
    "                        '10Y07ZR', '10Y07ZS', '10Y07ZT', '10Y07ZV', '10Y07ZY', '1160', \n",
    "                        '1169', '3350', '3351', '3352', '336', '3751', '4100', '4101', \n",
    "                        '4102', '4103', '4104', '4106', '4107', '4108', '4109', '4191', \n",
    "                        '4194', '4697', '4974', '5051', '5059', '5280', '5282', '5283', \n",
    "                        '5284', '5285', '5286', '5553', '5561', '5569', '6353', '6592',\n",
    "                        '8256', '8258', '8375', '8377', '8664', '5855']\n",
    "\n",
    "    hadm_ids_w_transplants = diagnoses_icd[diagnoses_icd.icd_code.isin(TRANSPLANT_CODES)].hadm_id.unique()\n",
    "    stay_ids_w_transplants = cohort[cohort.hadm_id.isin(hadm_ids_w_transplants)].stay_id.unique()\n",
    "    df['transplant'] = 0\n",
    "    transplant_indices = df[df.stay_id.isin(stay_ids_w_transplants)].index\n",
    "    df.loc[transplant_indices, 'transplant'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction == 'deceased':\n",
    "    stay_to_hadm_id = dict(zip(cohort.stay_id, cohort.hadm_id))\n",
    "    hadm_id_to_death_var = dict(zip(cohort.hadm_id, admissions.deathtime))\n",
    "    stay_id_to_death_var = {stay_id: (1 if isinstance((hadm_id_to_death_var[hadm_id]), str) else 0) \\\n",
    "                            for stay_id, hadm_id in stay_to_hadm_id.items()}\n",
    "\n",
    "    df[\"deceased\"] = df[\"stay_id\"].map(stay_id_to_death_var)    \n",
    "    # df.to_csv('melanoma_allFrames_w_death.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction == 'ventilated':\n",
    "    vent = procs[procs['itemid'] == 225792]\n",
    "    ventilated_stay_ids = vent.stay_id.unique()\n",
    "    df['ventilated'] = 0\n",
    "\n",
    "    ventilated_indices = df[df.stay_id.isin(ventilated_stay_ids)].index\n",
    "    df.loc[ventilated_indices, 'ventilated'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ethnicity, marital status, insurance\n",
    "df_subject_ids = cohort[cohort.stay_id.isin(df.stay_id.unique())].subject_id.unique()\n",
    "\n",
    "stay_to_sub_id_dict = dict()\n",
    "for sid in df.stay_id.unique():\n",
    "    stay_to_sub_id_dict[sid] = cohort[cohort.stay_id == sid].subject_id.unique()[0]\n",
    "\n",
    "sub_id_to_race_dict = dict()\n",
    "sub_id_to_marital_stat_dict = dict()\n",
    "sub_id_to_insurance_dict = dict()\n",
    "\n",
    "for sid in df_subject_ids:\n",
    "    sub_id_to_race_dict[sid] = admissions[admissions.subject_id == sid].ethnicity.unique()[0]\n",
    "    sub_id_to_marital_stat_dict[sid] = admissions[admissions.subject_id == sid].marital_status.unique()[0]\n",
    "    sub_id_to_insurance_dict[sid] = admissions[admissions.subject_id == sid].insurance.unique()[0]\n",
    "\n",
    "df['subject_id'] = df[\"stay_id\"].map(stay_to_sub_id_dict)\n",
    "df['ethnicity'] = df[\"subject_id\"].map(sub_id_to_race_dict)\n",
    "df['marital_status'] = df[\"subject_id\"].map(sub_id_to_marital_stat_dict)\n",
    "df['insurance'] = df[\"subject_id\"].map(sub_id_to_insurance_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a binary classifier (gradient boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COLS = ['anchor_age', 'patientweight', 'los', 'gender',\n",
    "       'alb', 'aniongap', 'bun', 'crp', 'ca', 'chloride', 'creatinine',\n",
    "       'fibrinogen', 'glucose', 'hgb', 'k', 'mg', 'na', 'p', 'platelets',\n",
    "       'troponin', 'wbc', 'apneainterval', 'artco2p', 'arto2p', 'expratio',\n",
    "       'hr', 'inspratio', 'insptime', 'nibpd', 'nibpm', 'nibps', 'pip', 'rr',\n",
    "       'spo2', 'temp', 'urine', 'vm', 'vt', 'betablockers', 'ca-iv',\n",
    "       'ca-noniv', 'cablockers', 'dextrose', 'fluids', 'insulin', 'k-iv',\n",
    "       'hours-k-iv', 'loopdiuretics', 'mg-iv', 'mg-noniv', 'hours-mg-noniv',\n",
    "       'p-iv', 'p-noniv', 'pnutrition', 'ponutrition', 'packedrbc',\n",
    "       'paralytics', 'sedation', 'tpnutrition', 'vasoactive', 'vasopressors',\n",
    "       'hours-p-iv']\n",
    "\n",
    "pred_var = prediction\n",
    "\n",
    "X = df[TRAIN_COLS]\n",
    "X = X.fillna(-1)\n",
    "y = df[pred_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29701, 62) (4243, 62) (8486, 62)\n",
      "(29701, 70) (4243, 70) (8486, 70)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train(70%)/validation(10%)/test(20%)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=0)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "df_trainval, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.125, random_state=0)\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of positive outcome: 36.57\n"
     ]
    }
   ],
   "source": [
    "# Percentage of positive outcome (calculated on the training set)\n",
    "print('% of positive outcome: {:.2f}'.format(y_train.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a hyperparameter search and find the best set of hyperparameters\n",
    "if False: \n",
    "    param_grid = {\n",
    "        'max_depth': [10, 50, 100],\n",
    "        'min_samples_split': [2, 4, 8],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': np.logspace(-4, 0, 10),\n",
    "    }\n",
    "\n",
    "    estimator = GradientBoostingClassifier(n_estimators=50, random_state=0)\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    clf.score(X_val, y_val)\n",
    "\n",
    "# Use the best set of hyperparameters\n",
    "else:\n",
    "    optimal_params = {'learning_rate': 0.359, \n",
    "                      'max_depth': 10,\n",
    "                      'n_estimators': 50,\n",
    "                      'min_samples_split': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set protected attribute\n",
    "group1 = 'ethnicity'; group2 = 'gender'\n",
    "groups1 = ['AMERICAN INDIAN/ALASKA NATIVE', 'ASIAN', 'BLACK/AFRICAN AMERICAN',\n",
    "           'HISPANIC/LATINO', 'WHITE']\n",
    "groups2 = set(df_test[group2])\n",
    "if np.nan in groups2: groups2.remove(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model N times\n",
    "N = 10\n",
    "results_dict = {}\n",
    "\n",
    "for seed in range(N):\n",
    "    results_dict[seed] = {}\n",
    "\n",
    "    # Train the classifier with a particular random seed\n",
    "    optimal_params['random_state'] = seed\n",
    "    clf = GradientBoostingClassifier(**optimal_params).fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate overall accuracy, precision, and recall\n",
    "    overall_accuracy = sklearn.metrics.accuracy_score(y_test, pred)*100\n",
    "    overall_precision = sklearn.metrics.precision_score(y_test, pred)*100\n",
    "    overall_recall = sklearn.metrics.recall_score(y_test, pred)*100\n",
    "\n",
    "    # Add overall results to the dictionary\n",
    "    results_dict[seed]['overall_accuracy'] = overall_accuracy\n",
    "    results_dict[seed]['overall_precision'] = overall_precision\n",
    "    results_dict[seed]['overall_recall'] = overall_recall\n",
    "\n",
    "    # Calculate per-group results\n",
    "    group_list = []\n",
    "    count_list = []\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    tn_list = []\n",
    "    fn_list = []\n",
    "    accuracy_list = []\n",
    "    ppv_list = []\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    fnr_list = []\n",
    "    gtpostrain_list = []\n",
    "    gtpostest_list = []\n",
    "    predpostest_list = []\n",
    "    ba_list = []\n",
    "    ba_ind_list = []\n",
    "    for g1 in sorted(groups1):\n",
    "        for g2 in sorted(groups2):\n",
    "\n",
    "            idx = np.logical_and(df_test[group1]==g1, df_test[group2]==g2)\n",
    "            count = idx.sum()\n",
    "            baserate = y_test[idx].mean()*100\n",
    "\n",
    "            # Calculate various metrics\n",
    "            tp = np.logical_and(y_test[idx]==1, pred[idx]==1).sum()\n",
    "            fp = np.logical_and(y_test[idx]==0, pred[idx]==1).sum()\n",
    "            tn = np.logical_and(y_test[idx]==0, pred[idx]==0).sum()\n",
    "            fn = np.logical_and(y_test[idx]==1, pred[idx]==0).sum()\n",
    "            accuracy = (tp+tn)/(tp+fp+tn+fn) * 100\n",
    "            ppv = tp/(tp+fp) * 100 # precision, used to calculate predictive parity\n",
    "            tpr = tp/(tp+fn) * 100 # recall, also used to calculate equalized opportunity and odds \n",
    "            fpr = fp/(fp+tn) * 100 # used to calculate equalized odds\n",
    "            fnr = fn/(fn+tp) * 100 # used to calculate equalized odds\n",
    "            po = (tp+fp)/(tp+fp+tn+fn) * 100 # used to calculate demographic parity\n",
    "\n",
    "            # Calculate bias amplification\n",
    "            idx_train = np.logical_and(df_train[group1]==g1, df_train[group2]==g2)\n",
    "            delta = (pred[idx]==1).mean() - (y_train[idx_train]==1).mean()\n",
    "            ind = np.logical_and(idx_train, y_train==1).mean() > idx_train.mean()*(y_train==1).mean()\n",
    "            baserate_train = y_train[idx_train].mean()*100\n",
    "            ba = (ind*delta + (~ind)*(-delta)) * 100\n",
    "\n",
    "            if False:\n",
    "                print()\n",
    "                print(g1, g2)\n",
    "                print('- Count', count)\n",
    "                print('- TP {}, FP {}, TN {}, FN {}'.format(tp, fp, tn, fn))\n",
    "                print('- Accuracy {:.2f}'.format(accuracy))\n",
    "                print('- PPV {:.2f}'.format(ppv))\n",
    "                print('- TPR {:.2f}'.format(tpr))\n",
    "                print('- FPR {:.2f}'.format(fpr))\n",
    "                print('- FNR {:.2f}'.format(fnr))\n",
    "                print('- GT positive outcome (train) {:.2f}'.format(baserate_train))\n",
    "                print('- GT positive outcome (test) {:.2f}'.format(baserate))\n",
    "                print('- Predicted positive outcome (test) {:.2f}'.format(po))\n",
    "                print('- Bias amplification {:.2f}'.format(ba))\n",
    "\n",
    "            group_list.append('{}-{}'.format(g1, g2))\n",
    "            count_list.append(count)\n",
    "            tp_list.append(tp)\n",
    "            fp_list.append(fp)\n",
    "            tn_list.append(tn)\n",
    "            fn_list.append(fn)\n",
    "            accuracy_list.append(accuracy)\n",
    "            ppv_list.append(ppv)\n",
    "            tpr_list.append(tpr)\n",
    "            fpr_list.append(fpr)\n",
    "            fnr_list.append(fnr)\n",
    "            gtpostrain_list.append(baserate_train)\n",
    "            gtpostest_list.append(baserate)\n",
    "            predpostest_list.append(po)\n",
    "            ba_list.append(ba)\n",
    "            ba_ind_list.append(ind)\n",
    "\n",
    "    # Add per-group results to the dictionary\n",
    "    results_dict[seed]['group_list'] = group_list\n",
    "    results_dict[seed]['count_list'] = count_list\n",
    "    results_dict[seed]['tp_list'] = tp_list\n",
    "    results_dict[seed]['fp_list'] = fp_list\n",
    "    results_dict[seed]['tn_list'] = tn_list\n",
    "    results_dict[seed]['fn_list'] = fn_list\n",
    "    results_dict[seed]['accuracy_list'] = accuracy_list\n",
    "    results_dict[seed]['ppv_list'] = ppv_list\n",
    "    results_dict[seed]['tpr_list'] = tpr_list\n",
    "    results_dict[seed]['fpr_list'] = fpr_list\n",
    "    results_dict[seed]['fnr_list'] = fnr_list\n",
    "    results_dict[seed]['gtpostrain_list'] = gtpostrain_list\n",
    "    results_dict[seed]['gtpostest_list'] = gtpostest_list\n",
    "    results_dict[seed]['predpostest_list'] = predpostest_list\n",
    "    results_dict[seed]['ba_list'] = ba_list\n",
    "    results_dict[seed]['ba_ind_list'] = ba_ind_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if False:\n",
    "    if task == 3: pickle.dump(results_dict, open('kidney_results.pkl', 'wb+'))\n",
    "    if task == 4: pickle.dump(results_dict, open('ventilation_results.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print mean and standard error for overall accuracy/precision/recall\n",
    "if False:\n",
    "    metrics = ['overall_accuracy', 'overall_precision', 'overall_recall']\n",
    "    for metric in metrics:\n",
    "        mean = np.mean([results_dict[seed][metric] for seed in range(N)])\n",
    "        sem = scipy.stats.sem([results_dict[seed][metric] for seed in range(N)], ddof=N-1)\n",
    "        print('{}: {:.2f} $\\pm$ {:.2f}'.format(metric, mean, sem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print mean and standard error for metric\n",
    "if False:\n",
    "    metric = 'ba_list'\n",
    "    print(metric)\n",
    "    print()\n",
    "\n",
    "    result = np.stack([results_dict[seed][metric] for seed in range(N)])\n",
    "    string = ''\n",
    "    for group in range(6):\n",
    "        mean = np.nanmean(result[:, group])\n",
    "        sem = scipy.stats.sem(result[:, group], ddof=N-1)\n",
    "        string += ' & {:.2f} $\\pm$ {:.2f}'.format(mean, sem)\n",
    "    print(string + ' \\\\\\\\')\n",
    "\n",
    "    print()\n",
    "\n",
    "    string = ''\n",
    "    for group in range(6, 10):\n",
    "        mean = np.nanmean(result[:, group])\n",
    "        sem = scipy.stats.sem(result[:, group], ddof=N-1)\n",
    "        string += ' & {:.2f} $\\pm$ {:.2f}'.format(mean, sem)\n",
    "    print(string + ' & \\multicolumn{2}{c}{} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
